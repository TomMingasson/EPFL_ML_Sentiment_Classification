{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glove import Glove\n",
    "from glove import Corpus\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re \n",
    "       \n",
    "def print_dict(dct):\n",
    "    for item, amount in dct.item():\n",
    "        print(\"{} ({})\".format(item, amount))\n",
    "\n",
    "def read_vocab_cut(filename):\n",
    "    \n",
    "    list_ = []\n",
    "    with open(filename, 'r', encoding='utf-8-sig') as datafile:\n",
    "        for line in datafile:\n",
    "            list_.append(line.strip()) \n",
    "    return list_\n",
    "\n",
    "def read_vocab(filename):\n",
    "    \n",
    "    dict_ = {}\n",
    "    with open(filename, 'r', encoding='utf-8-sig') as datafile:\n",
    "        for line in datafile:\n",
    "            occurence = line.strip().split(' ')[0]\n",
    "            word = line.strip().split(' ')[1]\n",
    "            dict_[word] = int(occurence)\n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1073741819"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r'D:/Documents/etudes/epfl/MA1/cours/MachineLearning/Project2/data/twitter_datasets_epfl/full/')\n",
    "os.system('build_vocab.sh')\n",
    "os.system('cut_vocab.sh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'D:\\Documents\\etudes\\epfl\\MA1\\cours\\MachineLearning\\Project2\\data\\twitter_datasets_stanford')\n",
    "\n",
    "glove_stanford = Glove.load_stanford('glove.twitter.27B.25d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('you', 0.94488276683023054),\n",
       " ('baby', 0.94254297070972637),\n",
       " ('dream', 0.92670410521870239),\n",
       " ('miss', 0.92469083211782976)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_stanford.most_similar(\"love\", number=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glove_stanford' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8a1a54e9656a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mword_filtered\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mword_filtered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_filtered\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;34m\"'\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_filtered\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mword_filtered\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mword_filtered\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mglove_stanford\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mword_filtered\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlacking_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                     \u001b[0mlacking_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_filtered\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocab_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'glove_stanford' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r'D:/Documents/etudes/epfl/MA1/cours/MachineLearning/Project2/data/twitter_datasets_epfl/full/')\n",
    "\n",
    "# read epfl data sets\n",
    "vocab_all = read_vocab('vocab_all_full_processed.txt')\n",
    "vocab_all_cut = read_vocab_cut('vocab_cut_all_full_processed.txt')\n",
    "\n",
    "lacking_words = {}\n",
    "for word in vocab_all_cut:\n",
    "        word_filtered = list(filter(None,  re.split(\"[ ]+\", word.strip())))\n",
    "        if word_filtered: \n",
    "            word_filtered = word_filtered[0]\n",
    "            if \"'\" in word_filtered and not word_filtered[0].isdigit() and not word_filtered in glove_stanford.dictionary:\n",
    "                if not word_filtered in lacking_words:\n",
    "                    lacking_words[word_filtered] = vocab_all[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y'all\t4061\n",
      "(':\t182\n",
      "i'l\t110\n",
      "a'f\t104\n",
      "y'know\t99\n",
      "u'r\t97\n",
      "i'am\t91\n",
      ":'\t85\n",
      "rt'ing\t57\n",
      "i'v\t57\n",
      "your'e\t39\n",
      "qur'an\t39\n",
      "dm'ed\t36\n",
      "you'l\t35\n",
      "f'n\t35\n",
      "dm'ing\t35\n",
      "rt'ed\t29\n",
      "did'nt\t27\n",
      "hold'em\t26\n",
      "iv'e\t25\n",
      "int'l\t25\n",
      "i'n\t25\n",
      "we'l\t24\n",
      "i'\t24\n",
      "bell'o\t24\n",
      "y'alls\t23\n",
      "u'v\t18\n",
      "u'l\t18\n",
      "mf'n\t18\n",
      "i'ts\t16\n",
      "gov't\t16\n",
      "did't\t16\n",
      "tell'em\t15\n",
      "it'l\t15\n",
      "ga'hoole\t15\n",
      "would'nt\t14\n",
      "we'r\t14\n",
      "could'nt\t14\n",
      "they'r\t13\n",
      "say'n\t13\n",
      "have'nt\t13\n",
      "cant't\t13\n",
      "t'other\t12\n",
      "sum'n\t12\n",
      "o'rings\t12\n",
      "s'pose\t11\n",
      "lol'ing\t11\n",
      "li'i\t11\n",
      "does'nt\t11\n",
      "cam'ron\t11\n",
      "we'v\t10\n",
      "na'night\t10\n",
      "m'love\t10\n",
      "m'f\t10\n",
      "he'l\t10\n",
      "good'n\t10\n",
      "chef'n\t10\n",
      "as'f\t10\n",
      "you'v\t9\n",
      "you'\t9\n",
      "is'nt\t9\n",
      "good'un\t9\n",
      "c'on\t9\n",
      ":'}\t9\n",
      "':\t9\n",
      "y'al\t8\n",
      "ya'know\t8\n",
      "t'f\t8\n",
      "good'night\t8\n",
      "fuck'n\t8\n",
      "dont't\t8\n",
      "doens't\t8\n",
      "were'nt\t7\n",
      "lol'ed\t7\n",
      "i'ont\t7\n",
      "insh'allah\t7\n",
      "im'a\t7\n",
      "g'zone\t7\n",
      "could't\t7\n",
      "<number>':\t7\n",
      "w'end\t6\n",
      "was'nt\t6\n",
      "see'n\t6\n",
      "rt'n\t6\n",
      "rt'in\t6\n",
      "rock'n'roll\t6\n",
      "rock'n\t6\n",
      "it'\t6\n",
      "go'n\t6\n",
      "get'em\t6\n",
      "ex'ample\t6\n",
      "d'u\t6\n",
      "don''t\t6\n",
      "com'on\t6\n",
      "c'n'c\t6\n",
      "ya'l\t5\n",
      "think'n\t5\n",
      "s'o\t5\n",
      "s'not\t5\n",
      "said'only\t5\n",
      "r'n'b\t5\n",
      "o'l\t5\n",
      "o'cake\t5\n",
      "n'w\t5\n",
      "mo'fo\t5\n",
      "m'n\t5\n",
      "m'lovely\t5\n",
      "look'n\t5\n",
      "it'a\t5\n",
      "i'on\t5\n",
      "g'n\t5\n",
      "eff'n\t5\n",
      "d'x\t5\n",
      "d'vita\t5\n",
      "dpgc'ology\t5\n",
      "dont'a\t5\n",
      "dj'ing\t5\n",
      "dick'ed\t5\n",
      "ca't\t5\n",
      "b'cause\t5\n",
      "amar'e\t5\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(\"{}\\t{}\".format(k, v) for k, v in lacking_words.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words in the epfl datasets BUT not present in stanford dictionary: 121\n",
      "number of words in the epfl datasets: 82405\n",
      "ratio in the epfl dataset: 0.1468357502578727\n"
     ]
    }
   ],
   "source": [
    "print('number of words in the epfl datasets BUT not present in stanford dictionary:', len(lacking_words))\n",
    "print('number of words in the epfl datasets:', len(vocab_all_cut))\n",
    "print('ratio in the epfl dataset:', len(lacking_words)/len(vocab_all_cut)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "freq_min = 50\n",
    "freq_max = np.inf\n",
    "\n",
    "lacking_words_sorted = sorted(lacking_words.copy().items(), key=operator.itemgetter(1), reverse = True)\n",
    "\n",
    "lacking_words_sorted_occ = []\n",
    "for k,v in lacking_words_sorted:\n",
    "    if v<freq_max and v>freq_min:\n",
    "        lacking_words_sorted_occ.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words in the epfl datasets BUT not present in stanford dictionary: 121\n",
      "number of words in the epfl datasets whose occurences is higher than , 50 : 82405\n",
      "ratio in the epfl dataset: 0.012135185971725018\n"
     ]
    }
   ],
   "source": [
    "print('number of words in the epfl datasets BUT not present in stanford dictionary:', len(lacking_words))\n",
    "print('number of words in the epfl datasets whose occurences is higher than ,', freq_min, ':', len(vocab_all_cut))\n",
    "print('ratio in the epfl dataset:', len(lacking_words_sorted_occ)/len(vocab_all_cut)*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
