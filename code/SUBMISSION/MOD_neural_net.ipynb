{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "TensorFlow imported\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# change path if necessary\n",
    "import sys\n",
    "my_path = r'C:\\Users\\utente\\Documents\\GitHub\\Project2'\n",
    "sys.path.insert(0,my_path + r'/code/Flavio')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "print('TensorFlow imported')\n",
    "\n",
    "import dictionary_helpers as dh\n",
    "\n",
    "# imports\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1193514\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r'C:\\Users\\utente\\Desktop\\data\\twitter_datasets_stanford')\n",
    "\n",
    "glove_dict = dh.build_glove_dict('glove.twitter.27B.25d.txt')\n",
    "\n",
    "print(len(glove_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.84705 , -1.0349  , -0.050419,  0.27164 , -0.58659 ,  0.99514 ,\n",
       "        0.25267 ,  1.6963  ,  0.10313 ,  0.80073 ,  0.74655 , -1.2667  ,\n",
       "       -4.036   , -0.22557 ,  0.16322 , -0.67015 , -0.64812 ,  0.010373,\n",
       "       -0.71889 , -0.74997 ,  0.24862 ,  0.10319 , -1.1732  ,  0.58196 ,\n",
       "        0.33846 ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_dict[',']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2280482\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.chdir(r'C:\\Users\\utente\\Desktop\\data\\twitter_datasets_epfl\\short')\n",
    "os.chdir(r'C:\\Users\\utente\\Desktop\\data\\twitter_datasets_epfl\\full')\n",
    "\n",
    "tweets = dh.build_tweets_matrix('all_full_processed.txt')\n",
    "        \n",
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'your'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  4.]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([])\n",
    "\n",
    "ciao=np.append(x,[1,4], axis=0)\n",
    "\n",
    "\n",
    "print(ciao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build averages+min+max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2280482, 75)\n",
      "[-0.1865898  0.236394   0.330381  -0.508172  -0.6313082 -0.1213108\n",
      "  1.256084  -0.3280116 -0.69894    0.190782  -0.289562   0.6072632 -5.21422\n",
      " -0.1280236 -0.211636   0.387608   0.2012168 -0.0554106 -0.053988\n",
      " -0.2539778  0.139662   0.33784   -0.0941022 -0.0355024  0.0722898\n",
      "  0.088601   0.59108    0.96279    0.19839   -0.092441   0.38567    2.1\n",
      "  0.066642  -0.49794    0.70047    0.43174    1.4784    -3.1591     0.15437\n",
      "  0.52152    0.86374    0.59554    0.88961    1.292      0.33094    0.4516\n",
      "  0.65844    0.54594    0.25164    0.2758    -0.45649   -0.20207   -0.12672\n",
      " -1.1372    -1.247     -0.40767    0.68192   -0.84406   -1.1467    -0.17034\n",
      " -0.87221   -0.013954  -5.995     -0.43106   -0.9154    -0.11314   -0.33704\n",
      " -0.47587   -0.84178   -0.84126   -0.4582    -0.16727   -0.47606   -0.39695\n",
      " -0.16073  ]\n"
     ]
    }
   ],
   "source": [
    "X = np.empty([len(tweets), feat_size*3])\n",
    "\n",
    "for i in range(len(tweets)):\n",
    "    feats = []\n",
    "    for j in range(len(tweets[i])):\n",
    "        if tweets[i][j] in glove_dict:\n",
    "            feats.append(glove_dict[tweets[i][j]])\n",
    "        else:\n",
    "            feats.append([0]*feat_size)\n",
    "\n",
    "    X[i]= np.hstack((np.mean(feats, axis=0), np.amax(feats, axis=0), np.amin(feats, axis=0)))\n",
    "\n",
    "print(X.shape)\n",
    "print(X[9])\n",
    "\n",
    "labels = []\n",
    "\n",
    "for i in range(1127644):\n",
    "    labels.append([1,0])\n",
    "for i in range(len(tweets)-1127644):\n",
    "    labels.append([0,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featSet = []\n",
    "\n",
    "for i in range(len(tweets)):\n",
    "    featSet.append([X[i,:].tolist(), labels[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.181188, 0.13865947419354843, 0.006939490322580643, -0.2190483870967742, -0.285460806451613, -0.03187148387096773, 1.0052274193548385, 0.250002870967742, -0.2340423870967742, 0.15821677419354843, -0.28707551612903226, 0.03712477419354836, -4.454083870967742, -0.3381245161290322, 0.06274933225806452, -0.18583977419354836, 0.21532859999999998, -0.44396445161290327, -0.3509488709677419, -0.382074064516129, 0.24983261935483878, 0.2606583967741935, -0.13827661290322582, 0.36713777741935477, -0.06539945161290323, 1.6477, 1.2678, 1.0937, 1.3254, 0.74877, 0.68537, 2.1431, 2.9738, 2.1517, 1.035, 0.53267, 1.3131, 0.0, 0.91787, 1.2789, 0.78043, 1.5676, 1.8308, 1.1712, 0.94705, 1.5811, 1.3184, 1.184, 1.5532, 1.3129, -1.0206, -0.87651, -0.71516, -1.6632, -1.0503, -0.70469, -1.6111, -1.1824, -1.3219, -1.0633, -1.6912, -2.9371, -6.2619, -2.0434, -1.6743, -0.94879, -1.0932, -1.1591, -2.0487, -1.6247, -1.0907, -0.57663, -1.1142, -1.2152, -0.77178], [1, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(featSet[190000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(X)\n",
    "del(labels)\n",
    "del(tweets)\n",
    "del(glove_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### crea train_x train_y test_x test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(featSet)\n",
    "\n",
    "featSet = np.array(featSet)\n",
    "\n",
    "testing_size = int(0.1*len(featSet))\n",
    "\n",
    "train_x = list(featSet[:,0][:-testing_size])\n",
    "train_y = list(featSet[:,1][:-testing_size])\n",
    "\n",
    "test_x = list(featSet[:,0][-testing_size:-1])\n",
    "test_y = list(featSet[:,1][-testing_size:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 75\n",
    "n_nodes_hl1 = 75\n",
    "n_nodes_hl2 = 30\n",
    "n_nodes_hl3 = 15\n",
    "\n",
    "n_classes = 2\n",
    "batch_size = 250\n",
    "\n",
    "\n",
    "# input feature size = 50feats x 20words = 2000 \n",
    "x = tf.placeholder('float', [None, n_inputs])\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "#DEFINITION OF THE NETWORK:\n",
    "#1) random initialization of the various layer with respectives dimensions, the layer is a DICTIONARY of weigths and biases\n",
    "#2) application of weights and biases and activation function to each layer\n",
    "\n",
    "\n",
    "def neural_network_model(data):\n",
    "    # input_data * weights + biases\n",
    "\n",
    "    #tf.random_normal returns an array of the specified dimension filled with randomly distributed numbers\n",
    "    #tf.Variable = object to be created that works in the tensorflow session, just a wrapper.\n",
    "    hidden_l1 = {'weights': tf.Variable(tf.random_normal([n_inputs, n_nodes_hl1])),\n",
    "    'biases': tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "    hidden_l2 = {'weights': tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "    'biases': tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "    hidden_l3 = {'weights': tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "    'biases': tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "    output_l  = {'weights': tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "    'biases': tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "    l1 = tf.add(tf.matmul(data, hidden_l1['weights']), hidden_l1['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1, hidden_l2['weights']), hidden_l2['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2, hidden_l3['weights']), hidden_l3['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "\n",
    "    output = tf.add(tf.matmul(l3, output_l['weights']), output_l['biases'])\n",
    "    \n",
    "    return output\n",
    "\n",
    "def train_neural_network(x):\n",
    "    prediction = neural_network_model(x)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))   # v1.0 changes\n",
    "    # optimizer value = 0.001, Adam similar to SGD\n",
    "    # everything is updating through this optimizer, i don't know how.... problem!? maybe better not to know\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    epochs_no = 50\n",
    " \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())   # v1.0 changes\n",
    "  \n",
    "        # training\n",
    "        for epoch in range(epochs_no):\n",
    "            epoch_loss = 0\n",
    "\n",
    "            #batching\n",
    "            i = 0\n",
    "            while i < len(train_x):\n",
    "                start = i\n",
    "                end = i+batch_size\n",
    "                \n",
    "                batch_x = np.array(train_x[start:end])\n",
    "                batch_y = np.array(train_y[start:end])\n",
    "                \n",
    "                _, c = sess.run([optimizer, cost], feed_dict = {x: batch_x, y: batch_y})\n",
    "                # code that optimizes the weights & biases\n",
    "                epoch_loss += c\n",
    "                i+=batch_size\n",
    "                \n",
    "            print('Epoch', epoch+1, 'completed out of', epochs_no, 'loss:', epoch_loss)\n",
    "\n",
    "        # testing\n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print('Accuracy:', accuracy.eval({x: test_x, y: test_y}))\n",
    "\n",
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTTTTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_neural_network():\n",
    "    prediction = neural_network_model(x)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "      \n",
    "        for epoch in range(hm_epochs):\n",
    "            try:\n",
    "                saver.restore(sess,\"model.ckpt\")\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "            epoch_loss = 0\n",
    "\n",
    "\n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        feature_sets = []\n",
    "        labels = []\n",
    "        counter = 0\n",
    "        with open('processed-test-set.csv', buffering=20000) as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    features = list(eval(line.split('::')[0]))\n",
    "                    label = list(eval(line.split('::')[1]))\n",
    "\n",
    "                    #print(features)\n",
    "                    #print(label)\n",
    "\n",
    "                    feature_sets.append(features)\n",
    "                    labels.append(label)\n",
    "                    counter += 1\n",
    "                except:\n",
    "                    pass\n",
    "        print('Tested',counter,'samples.')\n",
    "\n",
    "        test_x = np.array(feature_sets)\n",
    "        test_y = np.array(labels)\n",
    "        print('Accuracy:',accuracy.eval({x:test_x, y:test_y}))\n",
    "\n",
    "\n",
    "test_neural_network()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
