{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras imported\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# change path if necessary\n",
    "import sys\n",
    "my_path = r'C:\\Users\\utente\\Documents\\GitHub\\Project2'\n",
    "sys.path.insert(0,my_path + r'/code/Flavio')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "print('TensorFlow imported')\n",
    "\n",
    "import helpers as dh\n",
    "\n",
    "# imports\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "print('Keras imported')\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Remember to import the stanford dataset found in https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove_dict length =  1193514\n",
      "Features of ',' : \n",
      " [  3.22990000e-01  -5.82930000e-01  -2.33270000e-01   3.44090000e-01\n",
      "  -5.69340000e-01   1.32110000e+00  -8.62670000e-02   2.05260000e-01\n",
      "   5.83200000e-01   4.75160000e-01   4.57290000e-01  -2.60950000e-01\n",
      "  -2.95560000e+00   1.67340000e-01   4.65400000e-01   1.95300000e-03\n",
      "   3.11760000e-01  -4.92880000e-01  -4.95870000e-01  -2.89710000e-01\n",
      "   2.05190000e-01  -9.54020000e-02  -5.54190000e-01   2.57660000e-01\n",
      "  -5.55870000e-01  -2.47970000e+00  -5.64550000e-01  -1.79400000e-01\n",
      "   4.22600000e-01  -3.22630000e-01  -3.37600000e-01   4.13700000e-01\n",
      "  -2.61110000e-01   5.80890000e-01  -2.13750000e-01   1.29900000e-01\n",
      "  -1.00040000e-02  -1.43990000e-01  -1.83220000e-01  -3.06110000e-01\n",
      "  -1.44840000e+00   7.22220000e-01  -4.70500000e-01   4.09470000e-01\n",
      "   1.69630000e-01  -1.45480000e-01   1.04160000e-01   5.42010000e-02\n",
      "  -5.80360000e-01  -4.55060000e-01  -2.84750000e-01  -2.75170000e-02\n",
      "   1.06780000e-01  -1.04710000e-01  -6.76610000e-02  -1.60740000e-01\n",
      "  -3.44650000e-01  -1.41520000e-01   3.98620000e-01   2.99900000e-01\n",
      "  -2.84440000e-02   3.80920000e-01  -1.89220000e-01  -3.78400000e-02\n",
      "   2.74860000e-01   5.50350000e-02  -1.98070000e-01  -8.71730000e-01\n",
      "  -1.32530000e-01   3.95520000e-01   3.62900000e-01   4.60960000e-01\n",
      "  -1.75260000e-01   3.16820000e-01  -2.46220000e-01  -7.11990000e-01\n",
      "   1.77750000e-01  -9.36630000e-01  -1.00470000e-01   6.85900000e-01\n",
      "   5.42520000e-01  -7.28330000e-02  -1.93750000e-01   5.75100000e-01\n",
      "   2.79080000e-01   3.07200000e-01   1.88030000e-01   1.26440000e+00\n",
      "  -4.35400000e-01   4.95240000e-01  -5.19620000e-01  -6.22520000e-01\n",
      "   3.49500000e-02   2.02050000e-01  -1.84690000e-01  -5.44020000e-01\n",
      "   2.47500000e-01  -9.38830000e-01  -8.27710000e-02  -2.25740000e-01]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.chdir(r'C:\\Users\\utente\\Desktop\\data\\twitter_datasets_stanford')\n",
    "\n",
    "glove_dict = dh.build_glove_dict('glove.twitter.27B.100d.txt')\n",
    "\n",
    "print('glove_dict length = ' ,len(glove_dict))\n",
    "\n",
    "print('Features of \\',\\' : \\n', glove_dict[','])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting tweets from preprocessed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Remember to build the dataset with the preprocessing procedure in PRE_generate_txt_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets lenght:  2280482\n",
      "tweets sample (tweets[0]) : ['because' 'your' 'logic' 'is' 'so' 'dumb' ',' 'i' 'wo' 'not' 'even' 'crop'\n",
      " 'out' 'your' 'name' 'or' 'your' 'photo' '.' 'tsk' '.' '<url>']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.chdir(r'C:\\Users\\utente\\Desktop\\data\\twitter_datasets_epfl\\short')\n",
    "os.chdir(r'C:\\Users\\utente\\Desktop\\data\\twitter_datasets_epfl\\full')\n",
    "\n",
    "tweets = dh.build_tweets_matrix('all_full_processed.txt')\n",
    "        \n",
    "print('tweets lenght: ', len(tweets))\n",
    "\n",
    "print('tweets sample (tweets[0]) :', tweets[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build sentence features: averages+min+max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis script will produce a series of .txt files in order to save the splitted X matrix on the hard drive, this was done because \\nthe RAM of my PC is not big enough to contain the dictionary, the X matrix and the tweets at the same time\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The script will produce a series of .txt files in order to progressively save the splitted X matrix on the hard drive. \n",
    "This was done because the RAM of my PC is not big enough to contain the dictionary, the X matrix and the tweets at the same time.\n",
    "It also deletes the tweets that aren't useful anymore.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Parameters: set feature_size and delete_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import progressbar\n",
    "import gc\n",
    "\n",
    "tweetlen = len(tweets)\n",
    "\n",
    "bar = progressbar.ProgressBar(maxval=tweetlen, widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "\n",
    "feat_size = 100\n",
    "\n",
    "delete_win = 300000\n",
    "\n",
    "os.chdir(r'C:\\Users\\utente\\Desktop\\data\\twitter_datasets_epfl\\Xmatrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Building training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (300000, 300)\n",
      "X[9][5] =  0.210989071429   <class 'numpy.float64'>\n",
      "X[9] =  [ -3.23717643e-02  -6.47349643e-02   1.15616393e-01   6.34630107e-02\n",
      "  -7.29200714e-02   2.10989071e-01   7.12279643e-02  -1.52967857e-03\n",
      "   3.06660714e-02   4.91523571e-02  -5.25741786e-02  -8.16685714e-02\n",
      "  -3.12509179e+00  -8.01772857e-02  -6.99717500e-02  -1.28014184e-01\n",
      "   9.45028214e-02   5.05464286e-02   5.91748214e-02  -1.62991000e-01\n",
      "  -2.15696511e-01  -5.04910357e-02   1.15846496e-01   1.45981821e-01\n",
      "   1.86174143e-01  -6.54297357e-01  -9.52389286e-02   3.67039286e-03\n",
      "   5.35619393e-01   1.15220560e-01  -2.72081643e-01  -1.42230250e-01\n",
      "  -4.28881536e-01   2.30295539e-01   2.83070857e-01   1.01007036e-01\n",
      "   1.83310000e-02  -2.74448929e-03  -2.08975357e-02   3.20697357e-02\n",
      "  -8.95353250e-01   1.85881286e-01  -2.08809429e-01   1.87128429e-01\n",
      "   4.46815607e-01  -1.55061779e-01   9.36145714e-02   1.77959393e-01\n",
      "  -1.11808571e-01  -7.22662393e-02  -2.56615893e-01  -2.87647536e-01\n",
      "   8.78699286e-02   1.21987643e-01   1.02727679e-01   5.19896429e-02\n",
      "   9.26169000e-02   7.92960000e-02  -6.51885357e-02   2.09287893e-01\n",
      "   4.07188214e-02  -4.81066786e-02  -1.02975857e-01  -9.68800000e-02\n",
      "   2.89270964e-01  -2.95923179e-01  -5.97461786e-02  -3.19684032e-01\n",
      "   1.55357143e-02   1.07521607e-01   2.03286679e-01  -3.23781429e-02\n",
      "   1.97767071e-02   1.79092775e-02   3.63638214e-02  -2.65499643e-01\n",
      "  -6.14053511e-02  -1.40954393e-01  -3.89810357e-03  -6.99716786e-02\n",
      "   1.09135468e+00   1.65406121e-01  -3.06708514e-01  -3.24685000e-01\n",
      "   3.38819275e-01   6.99830750e-02  -9.96402857e-02   3.11492179e-01\n",
      "  -1.25581000e-01   3.84792286e-01  -1.73877714e-01  -4.34083214e-02\n",
      "   5.84395714e-02  -5.61315536e-02  -1.33454357e-01  -2.67212964e-01\n",
      "  -3.15598571e-02  -1.75983214e-01   4.71878571e-02   1.63242768e-01\n",
      "   8.53750000e-01   4.86310000e-01   7.13220000e-01   8.34140000e-01\n",
      "   8.20670000e-01   1.32110000e+00   7.51830000e-01   6.61780000e-01\n",
      "   5.83200000e-01   8.82960000e-01   6.06970000e-01   1.00970000e+00\n",
      "   1.06410000e-01   5.79220000e-01   4.78470000e-01   7.14770000e-01\n",
      "   9.67820000e-01   7.48380000e-01   1.31940000e+00   7.21470000e-01\n",
      "   7.48380000e-01   1.23900000e+00   6.99990000e-01   6.91110000e-01\n",
      "   1.16920000e+00   1.24090000e+00   4.95680000e-01   6.62120000e-01\n",
      "   1.04520000e+00   1.03390000e+00   7.15320000e-01   4.37270000e-01\n",
      "   7.71380000e-01   8.60640000e-01   1.75510000e+00   6.79890000e-01\n",
      "   4.73120000e-01   1.28420000e+00   8.85030000e-01   1.66690000e+00\n",
      "   3.84390000e-01   7.22220000e-01   8.89600000e-01   7.78430000e-01\n",
      "   1.39240000e+00   5.08340000e-01   7.83750000e-01   1.37360000e+00\n",
      "   5.23620000e-01   1.21860000e+00   1.26280000e+00   2.29500000e-01\n",
      "   6.75770000e-01   1.00560000e+00   9.92160000e-01   6.14950000e-01\n",
      "   9.09310000e-01   1.07540000e+00   7.47800000e-01   8.63710000e-01\n",
      "   6.65050000e-01   5.66300000e-01   6.59120000e-01   7.14190000e-01\n",
      "   1.65110000e+00   6.04170000e-01   1.12890000e+00   5.88850000e-01\n",
      "   8.06590000e-01   6.34900000e-01   1.00420000e+00   5.83240000e-01\n",
      "   7.35400000e-01   7.08290000e-01   6.24400000e-01   4.91300000e-01\n",
      "   7.30710000e-01   1.08090000e+00   7.19070000e-01   1.36260000e+00\n",
      "   2.36930000e+00   1.52670000e+00   2.15570000e-01   5.75100000e-01\n",
      "   8.13490000e-01   7.23250000e-01   4.83990000e-01   1.35490000e+00\n",
      "   6.25870000e-01   1.43060000e+00   3.47190000e-01   9.79060000e-01\n",
      "   7.01240000e-01   8.33010000e-01   1.09660000e+00   6.51260000e-01\n",
      "   4.86620000e-01   8.42140000e-01   1.26620000e+00   9.92830000e-01\n",
      "  -9.12310000e-01  -6.12450000e-01  -4.80410000e-01  -6.03900000e-01\n",
      "  -9.67720000e-01  -9.46140000e-01  -1.02050000e+00  -7.11130000e-01\n",
      "  -8.17970000e-01  -9.33390000e-01  -1.06820000e+00  -7.25980000e-01\n",
      "  -6.16510000e+00  -6.96640000e-01  -9.27650000e-01  -1.03820000e+00\n",
      "  -7.89260000e-01  -4.92880000e-01  -5.84590000e-01  -8.49750000e-01\n",
      "  -9.52450000e-01  -1.12760000e+00  -5.54190000e-01  -5.46500000e-01\n",
      "  -7.10530000e-01  -3.93550000e+00  -5.64550000e-01  -1.06870000e+00\n",
      "   0.00000000e+00  -5.98320000e-01  -1.02440000e+00  -1.10280000e+00\n",
      "  -1.36720000e+00  -3.29800000e-01  -5.78270000e-01  -4.95990000e-01\n",
      "  -5.54810000e-01  -7.88230000e-01  -6.23470000e-01  -8.39510000e-01\n",
      "  -2.32590000e+00  -1.07640000e+00  -8.92050000e-01  -4.05170000e-01\n",
      "  -9.20690000e-01  -8.35320000e-01  -6.58250000e-01  -7.48800000e-01\n",
      "  -5.80360000e-01  -1.04060000e+00  -1.18790000e+00  -1.15440000e+00\n",
      "  -6.73260000e-01  -5.04010000e-01  -5.59140000e-01  -6.36380000e-01\n",
      "  -7.35380000e-01  -4.97080000e-01  -6.82690000e-01  -7.03220000e-01\n",
      "  -5.63190000e-01  -9.88180000e-01  -8.62430000e-01  -1.17500000e+00\n",
      "  -5.72050000e-01  -1.32320000e+00  -9.76680000e-01  -8.71730000e-01\n",
      "  -5.61930000e-01  -7.56010000e-01  -5.60080000e-01  -1.03320000e+00\n",
      "  -6.33670000e-01  -9.45270000e-01  -6.21330000e-01  -1.27420000e+00\n",
      "  -1.49180000e+00  -9.36630000e-01  -8.70860000e-01  -1.67830000e+00\n",
      "  -7.39950000e-01  -5.47850000e-01  -1.29830000e+00  -1.42920000e+00\n",
      "  -3.21010000e-01  -6.07270000e-01  -5.38250000e-01  -7.21610000e-01\n",
      "  -1.19020000e+00  -3.09240000e-01  -7.26170000e-01  -7.50900000e-01\n",
      "  -2.64590000e-01  -9.35420000e-01  -1.32760000e+00  -1.02240000e+00\n",
      "  -7.95740000e-01  -9.38830000e-01  -9.61500000e-01  -6.63780000e-01]\n"
     ]
    }
   ],
   "source": [
    "X = np.empty([delete_win, feat_size*3])\n",
    "\n",
    "bar.start()\n",
    "\n",
    "k = 0\n",
    "i = 0\n",
    "\n",
    "while i < tweetlen:\n",
    "    \n",
    "    #everytime it enters in this if it creates a file for the X matrix to free RAM memory\n",
    "    if ((i%delete_win == 0) & (i!=0)):\n",
    "        tweets = np.delete(tweets, np.arange(0,delete_win) ,0)\n",
    "        strname = 'X'+ str(k) +'.txt'\n",
    "        np.savetxt(strname,X, delimiter=',')\n",
    "        k = k+1\n",
    "        X = np.empty([delete_win, feat_size*3])\n",
    "    \n",
    "    feats = []\n",
    "    for j in range(len(tweets[i-k*delete_win])):\n",
    "        if tweets[i-k*delete_win][j] in glove_dict:\n",
    "            feats.append(glove_dict[tweets[i-k*delete_win][j]])\n",
    "        else:\n",
    "            feats.append([0]*feat_size)\n",
    "\n",
    "    X[i-k*delete_win]= np.hstack((np.mean(feats, axis=0), np.amax(feats, axis=0), np.amin(feats, axis=0)))\n",
    "        \n",
    "    i = i+1\n",
    "    \n",
    "    bar.update(i)\n",
    "    \n",
    "bar.finish()\n",
    "    \n",
    "print('X shape: ' , X.shape)\n",
    "print('X[9][5] = ', X[9][5], ' ', type(X[9][5]))\n",
    "print('X[9] = ', X[9])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Label generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "halfway = 1127644 #For the full dataset\n",
    "#halfway = 90233    #For the short dataset\n",
    "\n",
    "for i in range(halfway):\n",
    "    labels.append(1)\n",
    "for i in range(len(tweets)-halfway):\n",
    "    labels.append(0)\n",
    "\n",
    "Y = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Deleting dictionary and tweets to free RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(tweets)\n",
    "del(glove_dict)\n",
    "del(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sample matrix rebuilt from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "X = np.empty([0,feat_size*3])\n",
    "\n",
    "#total number of files produced:\n",
    "final_num = 6\n",
    "\n",
    "for i in range (0,final_num+1):\n",
    "    print(i)\n",
    "    strf = 'X' + str(i) +'.txt'\n",
    "    X = np.concatenate((X,np.loadtxt(strf, delimiter = ',')))\n",
    "    \n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Definition of the network params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_input = feat_size*3\n",
    "l1 = feat_size*3\n",
    "l2 = feat_size*3\n",
    "l3 = feat_size\n",
    "l_output = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(l1, input_dim=l_input, activation='relu', kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "model.add(Dense(l2, activation='relu', kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "model.add(Dense(l3, activation='relu', kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "model.add(Dense(l_output, activation='sigmoid', kernel_initializer='glorot_uniform', bias_initializer='zeros'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X, Y, epochs=50, batch_size=250, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Importing dictionary and testing set (as before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'C:\\Users\\utente\\Desktop\\data\\twitter_datasets_stanford')\n",
    "\n",
    "glove_dict = dh.build_glove_dict('glove.twitter.27B.100d.txt')\n",
    "\n",
    "print('glove_dict length = ' ,len(glove_dict))\n",
    "\n",
    "print('Features of \\',\\' : \\n', glove_dict[','])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.chdir(r'C:\\Users\\utente\\Desktop\\data\\twitter_datasets_epfl\\short')\n",
    "os.chdir(r'C:\\Users\\utente\\Desktop\\data\\twitter_datasets_epfl')\n",
    "\n",
    "test_tweets = dh.build_tweets_matrix('test_data.txt')\n",
    "        \n",
    "print('tweets lenght: ', len(test_tweets))\n",
    "\n",
    "print('tweets sample (tweets[0]) :', test_tweets[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Build sentence features: averages+min+max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import progressbar\n",
    "\n",
    "bar = progressbar.ProgressBar(maxval=len(test_tweets), widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "\n",
    "feat_size = 100\n",
    "\n",
    "\n",
    "X = np.empty([len(test_tweets), feat_size*3])\n",
    "\n",
    "bar.start()\n",
    "\n",
    "for i in range(len(test_tweets)):\n",
    "    feats = []\n",
    "    for j in range(len(test_tweets[i])):\n",
    "        if test_tweets[i][j] in glove_dict:\n",
    "            feats.append(glove_dict[test_tweets[i][j]])\n",
    "        else:\n",
    "            feats.append([0]*feat_size)\n",
    "\n",
    "    X[i]= np.hstack((np.mean(feats, axis=0), np.amax(feats, axis=0), np.amin(feats, axis=0)))\n",
    "\n",
    "    bar.update(i)\n",
    "    \n",
    "bar.finish()\n",
    "    \n",
    "print('X shape: ' , X.shape)\n",
    "print('X[9][5] = ', X[9][5], ' ', type(X[9][5]))\n",
    "print('X[9] = ', X[9])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(glove_dict)\n",
    "del(test_tweets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Labelling predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(predictions))\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] < 0.5:\n",
    "        predictions[i] = -1\n",
    "    else:\n",
    "        predictions[i] = 1\n",
    "\n",
    "print(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from create_csv_submission import create_csv_submission\n",
    "\n",
    "ids = range(1,10001)\n",
    "\n",
    "name = 'subNN100'\n",
    "create_csv_submission(ids, predictions, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
