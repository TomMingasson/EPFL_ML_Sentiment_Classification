{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re \n",
    "import os\n",
    "from helpers import build_glove_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additionnal usefuls fucntions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_dict(dct):\n",
    "    for item, amount in dct.item():\n",
    "        print(\"{} ({})\".format(item, amount))\n",
    "\n",
    "def read_vocab_cut(filename):\n",
    "    \n",
    "    list_ = []\n",
    "    with open(filename, 'r', encoding='utf-8-sig') as datafile:\n",
    "        for line in datafile:\n",
    "            list_.append(line.strip()) \n",
    "    return list_\n",
    "\n",
    "def read_vocab(filename):\n",
    "    \n",
    "    dict_ = {}\n",
    "    with open(filename, 'r', encoding='utf-8-sig') as datafile:\n",
    "        for line in datafile:\n",
    "            occurence = line.strip().split(' ')[0]\n",
    "            word = line.strip().split(' ')[1]\n",
    "            dict_[word] = int(occurence)\n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build vocabulary of the word in the full tweet collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adapt path\n",
    "os.chdir(r'D:/Documents/etudes/epfl/MA1/cours/MachineLearning/Project2/data/twitter_datasets_epfl/full/')\n",
    "\n",
    "# build vocab\n",
    "# ! adapt the filename to process in the shell functions !\n",
    "os.system('build_vocab.sh')\n",
    "os.system('cut_vocab.sh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Glove model from Stanford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# adapt path\n",
    "os.chdir(r'D:/Documents/etudes/epfl/MA1/cours/MachineLearning/Project2/data/twitter_datasets_stanford/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filename\n",
    "filename_glove_dict = 'glove.twitter.27B.25d.txt'\n",
    "\n",
    "# build glove embeddings dictionary\n",
    "glove = build_glove_dict(filename_glove_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the vocabulary generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'D:/Documents/etudes/epfl/MA1/cours/MachineLearning/Project2/data/twitter_datasets_epfl/full/')\n",
    "\n",
    "# read epfl data sets\n",
    "vocab_all = read_vocab('vocab_all_full_processed.txt')\n",
    "vocab_all_cut = read_vocab_cut('vocab_cut_all_full_processed.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the words in the stanford dictionnary and in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lacking_words = {}\n",
    "for word in vocab_all_cut:\n",
    "        word_filtered = list(filter(None,  re.split(\"[ ]+\", word.strip())))\n",
    "        if word_filtered: \n",
    "            word_filtered = word_filtered[0]\n",
    "            if \"'\" in word_filtered and not word_filtered[0].isdigit() and not word_filtered in glove_stanford.dictionary:\n",
    "                if not word_filtered in lacking_words:\n",
    "                    lacking_words[word_filtered] = vocab_all[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\".join(\"{}\\t{}\".format(k, v) for k, v in lacking_words.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('number of words in the epfl datasets BUT not present in stanford dictionary:', len(lacking_words))\n",
    "print('number of words in the epfl datasets:', len(vocab_all_cut))\n",
    "print('ratio in the epfl dataset:', len(lacking_words)/len(vocab_all_cut)*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
