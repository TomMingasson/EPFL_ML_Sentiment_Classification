{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# change path if necessary\n",
    "import sys\n",
    "my_path = r'Scrivania/Machine_Learning/Project_2/Project2'\n",
    "sys.path.insert(0,my_path + r'/code/COMMON')\n",
    "\n",
    "# imports\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'/home/ilaria/Scrivania/Machine_Learning/Project_2/Project2/code/COMMON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictionary_helpers import build_glove_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'/home/ilaria/Scrivania/Machine_Learning/Project_2/Project2/data/twitter_datasets_stanford/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename_glove_dict = 'glove.twitter.27B.25d.txt'\n",
    "\n",
    "glove = build_glove_dict(filename_glove_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'/home/ilaria/Scrivania/Machine_Learning/Project_2/Project2/data/stop_words/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File : stop_word_freq_min_100_ratio_marg_0.1.txt\n",
      "Number of stop words : 630\n"
     ]
    }
   ],
   "source": [
    "filename_stopwords = 'stop_word_freq_min_100_ratio_marg_0.1.txt'\n",
    "\n",
    "stop_words = []\n",
    "with open(filename_stopwords, 'r', encoding='utf-8-sig') as f:\n",
    "    for line in f:\n",
    "        stop_words.append(line.lstrip().split()[0])\n",
    "    del stop_words[-1]\n",
    "    \n",
    "print(\"File :\", filename_stopwords)\n",
    "print(\"Number of stop words :\", len(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load tfid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'D:/Documents/etudes/epfl/MA1/cours/MachineLearning/Project2/data/tfidf/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build tweet vector method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'/home/ilaria/Scrivania/Machine_Learning/Project_2/Project2/code/tom/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tweet_processings import build_tweet_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method to build tweet vector\n",
    "method = [\"mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build tweet vectors TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'/home/ilaria/Scrivania/Machine_Learning/Project_2/Project2/data/twitter_datasets_epfl/short')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'lisining to music' is an empty tweet.\n",
      "'off to burgerboys' is an empty tweet.\n",
      "'walmart with mommiiee' is an empty tweet.\n",
      "'off to walmart' is an empty tweet.\n",
      "'up with andy' is an empty tweet.\n",
      "'with deanza' is an empty tweet.\n",
      "'going to crowely' is an empty tweet.\n"
     ]
    }
   ],
   "source": [
    "# build positive tweet feature set\n",
    "X_pos = []\n",
    "\n",
    "with open('train_pos_processed.txt') as f:\n",
    "    for line in f:\n",
    "        tweet = line.lstrip().split()\n",
    "        tweet_vector = build_tweet_vector(tweet, glove, tfidf, stop_words, method)\n",
    "        if len(tweet_vector):\n",
    "            X_pos.append(tweet_vector)\n",
    "        \n",
    "X_pos = np.array(X_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'enough' is an empty tweet.\n",
      "'im thinking too much' is an empty tweet.\n",
      "'kgnbgtini hugmepleaseprince ~' is an empty tweet.\n",
      "'scoiled my tongue' is an empty tweet.\n",
      "'so insanly busy' is an empty tweet.\n",
      "'<number><number> philly' is an empty tweet.\n",
      "'willy willy willy willy' is an empty tweet.\n"
     ]
    }
   ],
   "source": [
    "# build negative tweet feature set\n",
    "X_neg = []\n",
    "\n",
    "with open('train_neg_processed.txt') as f:\n",
    "    for line in f:\n",
    "        tweet = line.lstrip().split()\n",
    "        tweet_vector = build_tweet_vector(tweet, glove, tfidf, stop_words, method)\n",
    "        if len(tweet_vector):\n",
    "            X_neg.append(tweet_vector)\n",
    "        \n",
    "X_neg = np.array(X_neg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "y_pos = np.ones(X_pos.shape[0])\n",
    "y_neg = -np.ones(X_neg.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of training samples\n",
    "N_samples_train = 50000\n",
    "\n",
    "# cut samples\n",
    "X_pos_cut = X_pos[:N_samples_train,:]\n",
    "X_neg_cut = X_neg[:N_samples_train,:]\n",
    "\n",
    "# cut targets\n",
    "y_pos_cut = y_pos[:N_samples_train]\n",
    "y_neg_cut = y_neg[:N_samples_train]\n",
    "\n",
    "# concatenate\n",
    "X_pos_neg = np.concatenate([X_pos_cut, X_neg_cut])\n",
    "y_pos_neg = np.concatenate([y_pos_cut, y_neg_cut])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set to \"True\" to standardize\n",
    "ifStandardize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "if ifStandardize:\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_pos_neg)\n",
    "    X_pos_neg = scaler.transform(X_pos_neg)\n",
    "    \n",
    "else: \n",
    "    \n",
    "    X_pos_neg = X_pos_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization SVM (gamma and C) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# data\n",
    "X = X_pos_neg\n",
    "y = y_pos_neg\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# range for hyperparameters\n",
    "C_range = np.logspace(-2, 10, 10)\n",
    "gamma_range = np.logspace(-9, 3, 10)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'gamma': gamma_range, 'C': C_range}]\n",
    "                    \n",
    "# define grid search CV\n",
    "clf = GridSearchCV(SVC(kernel='rbf'), tuned_parameters, cv=5, scoring= 'accuracy')\n",
    "\n",
    "# fit for every parameters combinations in grid search CV\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "    \n",
    "print()\n",
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "print(\"The model is trained on the full development set.\")\n",
    "print(\"The scores are computed on the full evaluation set.\")\n",
    "print()\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import Normalize\n",
    "\n",
    "# Utility function to move the midpoint of a colormap to be around\n",
    "# the values of interest.\n",
    "\n",
    "class MidpointNormalize(Normalize):\n",
    "\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(value, x, y))\n",
    "   \n",
    "scores = clf.cv_results_['mean_test_score'].reshape(len(C_range), len(gamma_range))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\n",
    "plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot, norm=MidpointNormalize(vmin=0.2, midpoint=0.92))\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('C')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\n",
    "plt.yticks(np.arange(len(C_range)), C_range)\n",
    "plt.title('Validation accuracy')\n",
    "plt.savefig(\"photo\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
