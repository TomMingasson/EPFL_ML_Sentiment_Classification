{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# change path if necessary\n",
    "import sys\n",
    "my_path = r'D:\\Documents\\etudes\\epfl\\MA1\\cours\\MachineLearning\\Project2'\n",
    "sys.path.insert(0,my_path + r'/code/COMMON')\n",
    "\n",
    "# imports\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dictionary_helpers import build_glove_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'D:/Documents/etudes/epfl/MA1/cours/MachineLearning/Project2/data/twitter_datasets_stanford/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename_glove_dict = 'glove.twitter.27B.25d.txt'\n",
    "\n",
    "glove = build_glove_dict(filename_glove_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'D:/Documents/etudes/epfl/MA1/cours/MachineLearning/Project2/data/stop_words/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File : stop_word_freq_min_100_ratio_marg_0.1.txt\n",
      "Number of stop words : 630\n"
     ]
    }
   ],
   "source": [
    "filename_stopwords = 'stop_word_freq_min_100_ratio_marg_0.1.txt'\n",
    "\n",
    "stop_words = []\n",
    "with open(filename_stopwords, 'r', encoding='utf-8-sig') as f:\n",
    "    for line in f:\n",
    "        stop_words.append(line.lstrip().split()[0])\n",
    "    del stop_words[-1]\n",
    "    \n",
    "print(\"File :\", filename_stopwords)\n",
    "print(\"Number of stop words :\", len(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load tfid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'D:/Documents/etudes/epfl/MA1/cours/MachineLearning/Project2/data/tfidf/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename_tfidf = 'tfidf.txt'\n",
    "\n",
    "tfidf = {}\n",
    "with open(filename_tfidf, 'r', encoding='utf-8-sig') as f:\n",
    "    next(f) # skip headers\n",
    "    for line in f:\n",
    "        word = line.strip().split()[0]\n",
    "        tf = float(line.strip().split()[1])\n",
    "        idf = float(line.strip().split()[2])\n",
    "        tfidf[word] = [tf, idf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build tweet vector method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'D:/Documents/etudes/epfl/MA1/cours/MachineLearning/Project2/code/tom/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tweet_processings import build_tweet_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method to build tweet vector\n",
    "method = [\"mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build tweet vectors TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'D:\\Documents\\etudes\\epfl\\MA1\\cours\\MachineLearning\\Project2\\data\\twitter_datasets_epfl\\short')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'lisining to music' is an empty tweet.\n",
      "'off to burgerboys' is an empty tweet.\n",
      "'walmart with mommiiee' is an empty tweet.\n",
      "'off to walmart' is an empty tweet.\n",
      "'up with andy' is an empty tweet.\n",
      "'with deanza' is an empty tweet.\n",
      "'going to crowely' is an empty tweet.\n"
     ]
    }
   ],
   "source": [
    "# build positive tweet feature set\n",
    "X_pos = []\n",
    "\n",
    "with open('train_pos_processed.txt') as f:\n",
    "    for line in f:\n",
    "        tweet = line.lstrip().split()\n",
    "        tweet_vector = build_tweet_vector(tweet, glove, tfidf, stop_words, method)\n",
    "        if len(tweet_vector):\n",
    "            X_pos.append(tweet_vector)\n",
    "        \n",
    "X_pos = np.array(X_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'enough' is an empty tweet.\n",
      "'im thinking too much' is an empty tweet.\n",
      "'kgnbgtini hugmepleaseprince ~' is an empty tweet.\n",
      "'scoiled my tongue' is an empty tweet.\n",
      "'so insanly busy' is an empty tweet.\n",
      "'<number><number> philly' is an empty tweet.\n",
      "'willy willy willy willy' is an empty tweet.\n"
     ]
    }
   ],
   "source": [
    "# build negative tweet feature set\n",
    "X_neg = []\n",
    "\n",
    "with open('train_neg_processed.txt') as f:\n",
    "    for line in f:\n",
    "        tweet = line.lstrip().split()\n",
    "        tweet_vector = build_tweet_vector(tweet, glove, tfidf, stop_words, method)\n",
    "        if len(tweet_vector):\n",
    "            X_neg.append(tweet_vector)\n",
    "        \n",
    "X_neg = np.array(X_neg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels\n",
    "y_pos = np.ones(X_pos.shape[0])\n",
    "y_neg = -np.ones(X_neg.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of training samples\n",
    "N_samples_train = -1\n",
    "\n",
    "# cut samples\n",
    "X_pos_cut = X_pos[:N_samples_train,:]\n",
    "X_neg_cut = X_neg[:N_samples_train,:]\n",
    "\n",
    "# cut targets\n",
    "y_pos_cut = y_pos[:N_samples_train]\n",
    "y_neg_cut = y_neg[:N_samples_train]\n",
    "\n",
    "# concatenate\n",
    "X_pos_neg = np.concatenate([X_pos_cut, X_neg_cut])\n",
    "y_pos_neg = np.concatenate([y_pos_cut, y_neg_cut])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build tweet vectors TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'D:\\Documents\\etudes\\epfl\\MA1\\cours\\MachineLearning\\Project2\\data\\twitter_datasets_epfl\\short')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 25)\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "\n",
    "with open('test_data_no_id_processed.txt') as f:\n",
    "    for line in f:\n",
    "        tweet = line.lstrip().split()\n",
    "        tweet_vector = build_tweet_vector(tweet, glove, tfidf, stop_words, method)\n",
    "        if len(tweet_vector):\n",
    "            X_test.append(tweet_vector)\n",
    "        \n",
    "X_test = np.array(X_test) \n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set to \"True\" to standardize\n",
    "ifStandardize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "if ifStandardize:\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_pos_neg)\n",
    "    X_pos_neg = scaler.transform(X_pos_neg)\n",
    "    X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# init\n",
    "K = 100\n",
    "clf = KNeighborsClassifier(n_neighbors=K)\n",
    "\n",
    "# fit\n",
    "clf.fit(X_pos_neg, y_pos_neg) \n",
    "\n",
    "# predict\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'D:/Documents/etudes/epfl/MA1/cours/MachineLearning/Project2/data/submissions/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from create_csv_submission import create_csv_submission\n",
    "import time\n",
    "import datetime\n",
    "i = datetime.datetime.now()\n",
    "\n",
    "name = \"sub_\" + time.strftime(\"%d_%m_%Y\") +  \"_%sh_%smin\" % (i.hour, i.minute)\n",
    "#name = \"vrf\"\n",
    "ids_test = range(1, X_test.shape[0]+1)\n",
    "create_csv_submission(ids_test, y_pred, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
