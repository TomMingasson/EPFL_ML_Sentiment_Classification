{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# change path if necessary\n",
    "import sys\n",
    "my_path = r'D:\\Documents\\etudes\\epfl\\MA1\\cours\\MachineLearning\\Project2'\n",
    "sys.path.insert(0,my_path + r'/code/COMMON')\n",
    "\n",
    "# imports\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.signal import convolve\n",
    "\n",
    "def process_tweet(X_tweet, method):\n",
    "    \n",
    "    # mean of the features\n",
    "    if method is 'mean':\n",
    "        X_tweet_processed = np.mean(X_tweet, 0)\n",
    "    \n",
    "    return X_tweet_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glove import Corpus\n",
    "from glove import Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'D:/Documents/etudes/epfl/MA1/cours/MachineLearning/Project2/data/twitter_datasets_stanford/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Glove = Glove.load_stanford('glove.twitter.27B.25d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'D:/Documents/etudes/epfl/MA1/cours/MachineLearning/Project2/data/stop_words/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File : stop_word_freq_min_100_ratio_marg_0.2.txt\n",
      "Number of stop words : 1206\n"
     ]
    }
   ],
   "source": [
    "filename_stopwords = 'stop_word_freq_min_100_ratio_marg_0.2.txt'\n",
    "\n",
    "stop_words = []\n",
    "with open(filename_stopwords, 'r', encoding='utf-8-sig') as f:\n",
    "    for line in f:\n",
    "        stop_words.append(line.lstrip().split()[0])\n",
    "    del stop_words[-1]\n",
    "    \n",
    "print(\"File :\", filename_stopwords)\n",
    "print(\"Number of stop words :\", len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\",\n",
       " '&',\n",
       " '*',\n",
       " ',',\n",
       " ';',\n",
       " '\\\\',\n",
       " '_',\n",
       " '__',\n",
       " '`',\n",
       " '{',\n",
       " '}',\n",
       " '~',\n",
       " '<elong>',\n",
       " '<number>d',\n",
       " '<number>day',\n",
       " '<number>days',\n",
       " '<number>ish',\n",
       " '<number>th',\n",
       " '<number>years',\n",
       " '<number>yrs',\n",
       " '=',\n",
       " '->',\n",
       " 'a',\n",
       " 'aaaw',\n",
       " 'aah',\n",
       " 'aaron',\n",
       " 'abby',\n",
       " 'above',\n",
       " 'abs',\n",
       " 'abt',\n",
       " 'account',\n",
       " 'accounts',\n",
       " 'achieve',\n",
       " 'act',\n",
       " 'actor',\n",
       " 'actually',\n",
       " 'adam',\n",
       " 'add',\n",
       " 'address',\n",
       " 'adele',\n",
       " 'advance',\n",
       " 'after',\n",
       " 'again',\n",
       " 'ah',\n",
       " 'ahh',\n",
       " 'aim',\n",
       " 'aint',\n",
       " 'aj',\n",
       " 'alcohol',\n",
       " 'alex',\n",
       " 'algebra',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'allah',\n",
       " 'almost',\n",
       " 'already',\n",
       " 'although',\n",
       " 'am',\n",
       " 'amanda',\n",
       " 'amsterdam',\n",
       " 'and',\n",
       " 'andy',\n",
       " 'angel',\n",
       " 'anger',\n",
       " 'anniversary',\n",
       " 'announce',\n",
       " 'announced',\n",
       " 'annoy',\n",
       " 'anonymous',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'any',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'apa',\n",
       " 'apologies',\n",
       " 'apologize',\n",
       " 'app',\n",
       " 'appearance',\n",
       " 'apply',\n",
       " 'apprentice',\n",
       " 'april',\n",
       " 'arena',\n",
       " 'argentina',\n",
       " 'arguing',\n",
       " 'argument',\n",
       " 'arrive',\n",
       " 'arse',\n",
       " 'as',\n",
       " 'asap',\n",
       " 'ashley',\n",
       " 'asked',\n",
       " 'asks',\n",
       " 'asshole',\n",
       " 'at',\n",
       " 'atl',\n",
       " 'aunt',\n",
       " 'aunty',\n",
       " 'aus',\n",
       " 'austin',\n",
       " 'australia',\n",
       " 'aw',\n",
       " 'awards',\n",
       " 'aware',\n",
       " 'awh',\n",
       " 'awkward',\n",
       " 'aww',\n",
       " 'awwh',\n",
       " 'awwwh',\n",
       " 'babies',\n",
       " 'baby',\n",
       " 'back',\n",
       " 'bacon',\n",
       " 'badd',\n",
       " 'bak',\n",
       " 'bake',\n",
       " 'baked',\n",
       " 'bali',\n",
       " 'banana',\n",
       " 'barca',\n",
       " 'baseball',\n",
       " 'basically',\n",
       " 'basketball',\n",
       " 'bayern',\n",
       " 'bbq',\n",
       " 'bby',\n",
       " 'beach',\n",
       " 'beating',\n",
       " 'beats',\n",
       " 'bebe',\n",
       " 'because',\n",
       " 'bedroom',\n",
       " 'bedtime',\n",
       " 'before',\n",
       " 'bein',\n",
       " 'being',\n",
       " 'belfast',\n",
       " 'bella',\n",
       " 'bend',\n",
       " 'beside',\n",
       " 'beta',\n",
       " 'beth',\n",
       " 'bf',\n",
       " 'bieber',\n",
       " 'big',\n",
       " 'biggest',\n",
       " 'birmingham',\n",
       " 'biscuits',\n",
       " 'biting',\n",
       " 'blame',\n",
       " 'bliss',\n",
       " 'blonde',\n",
       " 'blown',\n",
       " 'bon',\n",
       " 'boo',\n",
       " 'boob',\n",
       " 'boobs',\n",
       " 'booth',\n",
       " 'born',\n",
       " 'bother',\n",
       " 'bounce',\n",
       " 'bow',\n",
       " 'brad',\n",
       " 'brave',\n",
       " 'break',\n",
       " 'breath',\n",
       " 'bright',\n",
       " 'brighton',\n",
       " 'bristol',\n",
       " 'bros',\n",
       " 'brother',\n",
       " 'brothers',\n",
       " 'brought',\n",
       " 'brownie',\n",
       " 'bruh',\n",
       " 'bryan',\n",
       " 'bs',\n",
       " 'bucket',\n",
       " 'bucks',\n",
       " 'bullshit',\n",
       " 'bum',\n",
       " 'bump',\n",
       " 'bun',\n",
       " 'bust',\n",
       " 'busy',\n",
       " 'button',\n",
       " 'buy',\n",
       " 'buying',\n",
       " 'buzz',\n",
       " 'bye',\n",
       " 'cafe',\n",
       " 'called',\n",
       " 'calling',\n",
       " 'calls',\n",
       " 'calories',\n",
       " 'cam',\n",
       " 'came',\n",
       " 'campus',\n",
       " 'candy',\n",
       " 'cardio',\n",
       " 'care',\n",
       " 'carlos',\n",
       " 'carnival',\n",
       " 'caroline',\n",
       " 'caught',\n",
       " 'cause',\n",
       " 'celebrity',\n",
       " 'certain',\n",
       " 'chachi',\n",
       " 'challenge',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'chase',\n",
       " 'cheat',\n",
       " 'checked',\n",
       " 'cheek',\n",
       " 'cheeks',\n",
       " 'cheer',\n",
       " 'cheese',\n",
       " 'cheesecake',\n",
       " 'chelsea',\n",
       " 'cher',\n",
       " 'cheryl',\n",
       " 'chicken',\n",
       " 'chin',\n",
       " 'chloe',\n",
       " 'choices',\n",
       " 'choir',\n",
       " 'chose',\n",
       " 'chris',\n",
       " 'christina',\n",
       " 'cinema',\n",
       " 'ck',\n",
       " 'claim',\n",
       " 'class',\n",
       " 'classes',\n",
       " 'cleaned',\n",
       " 'closet',\n",
       " 'clothes',\n",
       " 'clouds',\n",
       " 'clubs',\n",
       " 'cn',\n",
       " 'cod',\n",
       " 'coke',\n",
       " 'colour',\n",
       " 'colours',\n",
       " 'come',\n",
       " 'compared',\n",
       " 'concept',\n",
       " 'concert',\n",
       " 'concerts',\n",
       " 'confirm',\n",
       " 'confirmed',\n",
       " 'cont',\n",
       " 'contact',\n",
       " 'continue',\n",
       " 'convince',\n",
       " 'convinced',\n",
       " 'cook',\n",
       " 'cookie',\n",
       " 'cookies',\n",
       " 'cooking',\n",
       " 'cops',\n",
       " 'cousins',\n",
       " 'crack',\n",
       " 'craig',\n",
       " 'creepy',\n",
       " 'crew',\n",
       " 'crib',\n",
       " 'crowd',\n",
       " 'cuddling',\n",
       " 'cunt',\n",
       " 'curls',\n",
       " 'currently',\n",
       " 'cus',\n",
       " 'cuz',\n",
       " 'cyrus',\n",
       " 'da',\n",
       " 'dah',\n",
       " 'dan',\n",
       " 'dancers',\n",
       " 'dare',\n",
       " 'dates',\n",
       " 'dating',\n",
       " 'daughter',\n",
       " 'days',\n",
       " 'debate',\n",
       " 'decide',\n",
       " 'dee',\n",
       " 'delicious',\n",
       " 'dem',\n",
       " 'den',\n",
       " 'deny',\n",
       " 'describe',\n",
       " 'despite',\n",
       " 'dessert',\n",
       " 'details',\n",
       " 'dick',\n",
       " 'diff',\n",
       " 'difference',\n",
       " 'different',\n",
       " 'din',\n",
       " 'disco',\n",
       " 'discount',\n",
       " 'district',\n",
       " 'ditch',\n",
       " 'diva',\n",
       " 'diving',\n",
       " 'dms',\n",
       " 'do',\n",
       " 'doll',\n",
       " 'dollar',\n",
       " 'dollars',\n",
       " 'dong',\n",
       " 'dose',\n",
       " 'doubt',\n",
       " 'douche',\n",
       " 'dough',\n",
       " 'down',\n",
       " 'downtown',\n",
       " 'drake',\n",
       " 'drama',\n",
       " 'draw',\n",
       " 'drawing',\n",
       " 'dreaming',\n",
       " 'dreams',\n",
       " 'dress',\n",
       " 'drink',\n",
       " 'drinking',\n",
       " 'driving',\n",
       " 'drop',\n",
       " 'drug',\n",
       " 'drunk',\n",
       " 'dubai',\n",
       " 'dublin',\n",
       " 'ducks',\n",
       " 'dumb',\n",
       " 'easter',\n",
       " 'eat',\n",
       " 'eating',\n",
       " 'eats',\n",
       " 'edit',\n",
       " 'editing',\n",
       " 'effort',\n",
       " 'egg',\n",
       " 'eggs',\n",
       " 'eh',\n",
       " 'elf',\n",
       " 'emails',\n",
       " 'embrace',\n",
       " 'emily',\n",
       " 'enough',\n",
       " 'entered',\n",
       " 'entertain',\n",
       " 'esp',\n",
       " 'especially',\n",
       " 'essex',\n",
       " 'event',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyday',\n",
       " 'everything',\n",
       " 'everythings',\n",
       " 'ex',\n",
       " 'example',\n",
       " 'excitement',\n",
       " 'explain',\n",
       " 'eyebrows',\n",
       " 'face',\n",
       " 'facetime',\n",
       " 'factor',\n",
       " 'facts',\n",
       " 'fag',\n",
       " 'faith',\n",
       " 'fake',\n",
       " 'familiar',\n",
       " 'famous',\n",
       " 'fancy',\n",
       " 'fandom',\n",
       " 'far',\n",
       " 'faster',\n",
       " 'fatty',\n",
       " 'feelin',\n",
       " 'fest',\n",
       " 'fifa',\n",
       " 'fight',\n",
       " 'fights',\n",
       " 'fill',\n",
       " 'find',\n",
       " 'finished',\n",
       " 'finishing',\n",
       " 'first',\n",
       " 'flags',\n",
       " 'flight',\n",
       " 'flow',\n",
       " 'flowers',\n",
       " 'fly',\n",
       " 'flyers',\n",
       " 'for',\n",
       " 'found',\n",
       " 'france',\n",
       " 'freakin',\n",
       " 'free',\n",
       " 'fresh',\n",
       " 'freshman',\n",
       " 'fri',\n",
       " 'friendly',\n",
       " 'friendship',\n",
       " 'frm',\n",
       " 'fuck',\n",
       " 'fucker',\n",
       " 'fuckin',\n",
       " 'gaga',\n",
       " 'gain',\n",
       " 'gained',\n",
       " 'gal',\n",
       " 'gave',\n",
       " 'gay',\n",
       " 'genius',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'gf',\n",
       " 'ghetto',\n",
       " 'gift',\n",
       " 'gig',\n",
       " 'girlfriends',\n",
       " 'given',\n",
       " 'glasgow',\n",
       " 'glee',\n",
       " 'go',\n",
       " 'going',\n",
       " 'gomez',\n",
       " 'gona',\n",
       " 'gonna',\n",
       " 'goods',\n",
       " 'goose',\n",
       " 'gossip',\n",
       " 'got',\n",
       " 'grades',\n",
       " 'grandparents',\n",
       " 'grass',\n",
       " 'greg',\n",
       " 'grown',\n",
       " 'guess',\n",
       " 'guest',\n",
       " 'gunna',\n",
       " 'gusto',\n",
       " 'habit',\n",
       " 'hair',\n",
       " 'haircut',\n",
       " 'halfway',\n",
       " 'hands',\n",
       " 'hang',\n",
       " 'hangout',\n",
       " 'happens',\n",
       " 'hardcore',\n",
       " 'have',\n",
       " 'having',\n",
       " 'he',\n",
       " 'hearing',\n",
       " 'heaven',\n",
       " 'heck',\n",
       " 'heels',\n",
       " 'hell',\n",
       " 'help',\n",
       " 'helps',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'hide',\n",
       " 'higher',\n",
       " 'highschool',\n",
       " 'him',\n",
       " 'hips',\n",
       " 'his',\n",
       " 'hit',\n",
       " 'hitler',\n",
       " 'hold',\n",
       " 'holding',\n",
       " 'holland',\n",
       " 'hollie',\n",
       " 'hologram',\n",
       " 'honestly',\n",
       " 'honor',\n",
       " 'hook',\n",
       " 'hop',\n",
       " 'hoping',\n",
       " 'horny',\n",
       " 'hotel',\n",
       " 'hottest',\n",
       " 'how',\n",
       " 'hunger',\n",
       " 'hurry',\n",
       " 'hv',\n",
       " 'hve',\n",
       " 'hyper',\n",
       " 'i',\n",
       " 'ian',\n",
       " 'ice',\n",
       " 'idea',\n",
       " 'ideas',\n",
       " 'idiots',\n",
       " 'idol',\n",
       " 'ik',\n",
       " 'iknow',\n",
       " 'il',\n",
       " 'im',\n",
       " 'imagine',\n",
       " 'immediately',\n",
       " 'important',\n",
       " 'ina',\n",
       " 'incase',\n",
       " 'incredible',\n",
       " 'indonesia',\n",
       " 'ini',\n",
       " 'init',\n",
       " 'inn',\n",
       " 'insane',\n",
       " 'insecure',\n",
       " 'inspirational',\n",
       " 'instantly',\n",
       " 'instead',\n",
       " 'internship',\n",
       " 'interviews',\n",
       " 'introduce',\n",
       " 'invented',\n",
       " 'invite',\n",
       " 'invited',\n",
       " 'irish',\n",
       " 'is',\n",
       " 'it',\n",
       " 'italy',\n",
       " 'its',\n",
       " 'ja',\n",
       " 'jacob',\n",
       " 'jade',\n",
       " 'jam',\n",
       " 'jamie',\n",
       " 'jasmine',\n",
       " 'jay',\n",
       " 'jd',\n",
       " 'je',\n",
       " 'jenny',\n",
       " 'jerry',\n",
       " 'jessie',\n",
       " 'jesus',\n",
       " 'jo',\n",
       " 'job',\n",
       " 'joel',\n",
       " 'joey',\n",
       " 'joke',\n",
       " 'josh',\n",
       " 'joy',\n",
       " 'jst',\n",
       " 'juan',\n",
       " 'juice',\n",
       " 'julie',\n",
       " 'july',\n",
       " 'jump',\n",
       " 'just',\n",
       " 'justin',\n",
       " 'k',\n",
       " 'kaka',\n",
       " 'kan',\n",
       " 'kasi',\n",
       " 'kat',\n",
       " 'katie',\n",
       " 'keen',\n",
       " 'keeper',\n",
       " 'keith',\n",
       " 'kevin',\n",
       " 'kicks',\n",
       " 'kinda',\n",
       " 'kinds',\n",
       " 'km',\n",
       " 'knew',\n",
       " 'knicks',\n",
       " 'knw',\n",
       " 'ko',\n",
       " 'kyle',\n",
       " 'lagi',\n",
       " 'laid',\n",
       " 'lame',\n",
       " 'laughed',\n",
       " 'launch',\n",
       " 'laura',\n",
       " 'lauren',\n",
       " 'lay',\n",
       " 'laying',\n",
       " 'lean',\n",
       " 'learn',\n",
       " 'legit',\n",
       " 'leo',\n",
       " 'lesbian',\n",
       " 'less',\n",
       " 'lesson',\n",
       " 'letter',\n",
       " 'liar',\n",
       " 'lie',\n",
       " 'life',\n",
       " 'lifes',\n",
       " 'like',\n",
       " 'likely',\n",
       " 'lipstick',\n",
       " 'lisa',\n",
       " 'lit',\n",
       " 'little',\n",
       " 'liz',\n",
       " 'll',\n",
       " 'lloyd',\n",
       " 'local',\n",
       " 'looked',\n",
       " 'lool',\n",
       " 'looong',\n",
       " 'lord',\n",
       " 'loser',\n",
       " 'lovers',\n",
       " 'lush',\n",
       " 'lyk',\n",
       " 'made',\n",
       " 'main',\n",
       " 'makes',\n",
       " 'making',\n",
       " 'malaysia',\n",
       " 'mane',\n",
       " 'manila',\n",
       " 'mans',\n",
       " 'many',\n",
       " 'marathon',\n",
       " 'maria',\n",
       " 'maroon',\n",
       " 'massive',\n",
       " 'match',\n",
       " 'matching',\n",
       " 'mates',\n",
       " 'matt',\n",
       " 'mau',\n",
       " 'mcdonalds',\n",
       " 'me',\n",
       " 'mean',\n",
       " 'meaning',\n",
       " 'meg',\n",
       " 'megan',\n",
       " 'melbourne',\n",
       " 'melt',\n",
       " 'member',\n",
       " 'members',\n",
       " 'ment',\n",
       " 'mentions',\n",
       " 'menu',\n",
       " 'mess',\n",
       " 'messages',\n",
       " 'messing',\n",
       " 'mexican',\n",
       " 'michelle',\n",
       " 'mighty',\n",
       " 'mii',\n",
       " 'mile',\n",
       " 'min',\n",
       " 'mine',\n",
       " 'mins',\n",
       " 'minus',\n",
       " 'mixtape',\n",
       " 'mj',\n",
       " 'mo',\n",
       " 'mom',\n",
       " 'moments',\n",
       " 'moms',\n",
       " 'month',\n",
       " 'mood',\n",
       " 'more',\n",
       " 'morn',\n",
       " 'mostly',\n",
       " 'mother',\n",
       " 'mothers',\n",
       " 'motivated',\n",
       " 'mouth',\n",
       " 'movie',\n",
       " 'mr',\n",
       " 'mrs',\n",
       " 'msg',\n",
       " 'mt',\n",
       " 'mtv',\n",
       " 'much',\n",
       " 'muffin',\n",
       " 'muscles',\n",
       " 'music',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'n',\n",
       " 'na',\n",
       " 'naa',\n",
       " 'nails',\n",
       " 'named',\n",
       " 'nap',\n",
       " 'naps',\n",
       " 'narnia',\n",
       " 'nasty',\n",
       " 'necessary',\n",
       " 'needing',\n",
       " 'needs',\n",
       " 'negative',\n",
       " 'neighbor',\n",
       " 'neighbors',\n",
       " 'nerves',\n",
       " 'netflix',\n",
       " 'nevermind',\n",
       " 'new',\n",
       " 'newcastle',\n",
       " 'ni',\n",
       " 'nick',\n",
       " 'nikki',\n",
       " 'nina',\n",
       " 'ninja',\n",
       " 'nipples',\n",
       " 'noon',\n",
       " 'nope',\n",
       " 'normal',\n",
       " 'normally',\n",
       " 'norway',\n",
       " 'note',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'noww',\n",
       " 'nutella',\n",
       " 'nvm',\n",
       " 'nw',\n",
       " 'nxt',\n",
       " 'nyc',\n",
       " 'o_o',\n",
       " 'obsession',\n",
       " \"o'clock\",\n",
       " 'odd',\n",
       " 'off',\n",
       " 'offended',\n",
       " 'officially',\n",
       " 'often',\n",
       " 'ohwell',\n",
       " 'oi',\n",
       " 'older',\n",
       " 'olivia',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'ones',\n",
       " 'onto',\n",
       " 'oo',\n",
       " 'opportunity',\n",
       " 'or',\n",
       " 'orders',\n",
       " 'oreo',\n",
       " 'other',\n",
       " 'otherwise',\n",
       " 'out',\n",
       " 'outta',\n",
       " 'ova',\n",
       " 'own',\n",
       " 'packing',\n",
       " 'paid',\n",
       " 'pants',\n",
       " 'papa',\n",
       " 'park',\n",
       " 'parking',\n",
       " 'parties',\n",
       " 'partner',\n",
       " 'pass',\n",
       " 'pay',\n",
       " 'payday',\n",
       " 'paying',\n",
       " 'peek',\n",
       " 'pens',\n",
       " 'people',\n",
       " 'peoples',\n",
       " 'perfect',\n",
       " 'perfection',\n",
       " 'perform',\n",
       " 'performances',\n",
       " 'perfume',\n",
       " 'period',\n",
       " 'pete',\n",
       " 'phil',\n",
       " 'phillip',\n",
       " 'philly',\n",
       " 'photo',\n",
       " 'picked',\n",
       " 'pictures',\n",
       " 'pie',\n",
       " 'pierced',\n",
       " 'pinky',\n",
       " 'pint',\n",
       " 'piss',\n",
       " 'pj',\n",
       " 'place',\n",
       " 'planning',\n",
       " 'plans',\n",
       " 'play',\n",
       " 'played',\n",
       " 'playing',\n",
       " 'playlist',\n",
       " 'pls',\n",
       " 'pm',\n",
       " 'poem',\n",
       " 'pointless',\n",
       " 'points',\n",
       " 'pooh',\n",
       " 'pool',\n",
       " 'pop',\n",
       " 'popcorn',\n",
       " 'popped',\n",
       " 'popping',\n",
       " 'pops',\n",
       " 'pork',\n",
       " 'porn',\n",
       " 'pose',\n",
       " 'possible',\n",
       " 'post',\n",
       " 'posters',\n",
       " 'posts',\n",
       " 'potatoes',\n",
       " 'pour',\n",
       " 'pow',\n",
       " 'praise',\n",
       " 'pray',\n",
       " 'praying',\n",
       " 'precious',\n",
       " 'premiere',\n",
       " 'prepare',\n",
       " 'prepared',\n",
       " 'preparing',\n",
       " 'presence',\n",
       " 'pretend',\n",
       " 'princess',\n",
       " 'probably',\n",
       " 'profile',\n",
       " 'progress',\n",
       " 'prolly',\n",
       " 'promised',\n",
       " 'promises',\n",
       " 'promo',\n",
       " 'pub',\n",
       " 'pull',\n",
       " 'pulling',\n",
       " 'pun',\n",
       " 'punch',\n",
       " 'push',\n",
       " 'pushing',\n",
       " 'qt',\n",
       " 'que',\n",
       " 'queen',\n",
       " 'questions',\n",
       " 'quiet',\n",
       " 'quit',\n",
       " 'quiz',\n",
       " 'rachel',\n",
       " 'rae',\n",
       " 'randomly',\n",
       " 'rap',\n",
       " 'reached',\n",
       " 'reaching',\n",
       " 'reading',\n",
       " 'reads',\n",
       " 'real',\n",
       " 'recipe',\n",
       " 'reece',\n",
       " 'refreshing',\n",
       " 'register',\n",
       " 'regret',\n",
       " 'relay',\n",
       " 'remain',\n",
       " 'reminds',\n",
       " 'replace',\n",
       " 'replay',\n",
       " 'replied',\n",
       " 'reserve',\n",
       " 'resist',\n",
       " 'respond',\n",
       " 'rest',\n",
       " 'restaurant',\n",
       " 'result',\n",
       " 'retarded',\n",
       " 'reunion',\n",
       " 'revenge',\n",
       " 'ricky',\n",
       " 'ride',\n",
       " 'rides',\n",
       " 'riding',\n",
       " 'right',\n",
       " 'rihanna',\n",
       " 'rita',\n",
       " 'rite',\n",
       " 'rob',\n",
       " 'role',\n",
       " 'roll',\n",
       " 'romantic',\n",
       " 'route',\n",
       " 'routine',\n",
       " 'row',\n",
       " 'rugby',\n",
       " 'rule',\n",
       " 'run',\n",
       " 'runs',\n",
       " 'rush',\n",
       " 'ryan',\n",
       " 'ryt',\n",
       " 's',\n",
       " 'safely',\n",
       " 'said',\n",
       " 'salad',\n",
       " 'salon',\n",
       " 'sam',\n",
       " 'same',\n",
       " 'sana',\n",
       " 'sandwich',\n",
       " 'sandwiches',\n",
       " 'sara',\n",
       " 'sarah',\n",
       " 'saturday',\n",
       " 'saving',\n",
       " 'saw',\n",
       " 'says',\n",
       " 'score',\n",
       " 'scream',\n",
       " 'screamed',\n",
       " 'screaming',\n",
       " 'sean',\n",
       " 'seats',\n",
       " 'seconds',\n",
       " 'seein',\n",
       " 'seeing',\n",
       " 'sees',\n",
       " 'sending',\n",
       " 'sends',\n",
       " 'served',\n",
       " 'settle',\n",
       " 'sexual',\n",
       " 'sh',\n",
       " 'shades',\n",
       " 'shared',\n",
       " 'sheffield',\n",
       " 'shinee',\n",
       " 'ship',\n",
       " 'shirt',\n",
       " 'shit',\n",
       " 'shits',\n",
       " 'sho',\n",
       " 'shop',\n",
       " 'shops',\n",
       " 'shorts',\n",
       " 'shot',\n",
       " 'shots',\n",
       " 'shoulders',\n",
       " 'shouting',\n",
       " 'showers',\n",
       " 'shrimp',\n",
       " 'shud',\n",
       " 'shut',\n",
       " 'shyt',\n",
       " 'si',\n",
       " 'side',\n",
       " 'sign',\n",
       " 'silence',\n",
       " 'singers',\n",
       " 'sisters',\n",
       " 'sit',\n",
       " 'site',\n",
       " 'skool',\n",
       " 'sky',\n",
       " 'skype',\n",
       " 'slap',\n",
       " 'sleeps',\n",
       " 'slick',\n",
       " 'smart',\n",
       " 'smarter',\n",
       " 'smells',\n",
       " 'smoked',\n",
       " 'snacks',\n",
       " 'so',\n",
       " 'soccer',\n",
       " 'somebody',\n",
       " 'somehow',\n",
       " 'someones',\n",
       " 'something',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'son',\n",
       " 'soo',\n",
       " 'sophie',\n",
       " 'sound',\n",
       " 'sounded',\n",
       " 'sour',\n",
       " 'spaghetti',\n",
       " 'spamming',\n",
       " 'speaking',\n",
       " 'special',\n",
       " 'spelling',\n",
       " 'spend',\n",
       " 'spree',\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"happy\" in stop_words)\n",
    "print(\"another\" in stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build tweet vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'D:\\Documents\\etudes\\epfl\\MA1\\cours\\MachineLearning\\Project2\\data\\twitter_datasets_epfl\\short')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method to build tweet vector\n",
    "method = \"mean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build positive tweet feature set\n",
    "X_pos = []\n",
    "\n",
    "with open('train_pos_processed.txt') as f:\n",
    "    for line in f:\n",
    "        tweet = line.lstrip().split()\n",
    "        X_tweet = []\n",
    "        for word in tweet:\n",
    "            if word in Glove.dictionary and not word in stop_words:\n",
    "                word_vector = Glove.word_vectors[Glove.dictionary.get(word)]\n",
    "                X_tweet.append(word_vector)\n",
    "                \n",
    "        if X_tweet:\n",
    "            X_tweet_processed = process_tweet(X_tweet, method)\n",
    "            X_pos.append(X_tweet_processed)\n",
    "        \n",
    "X_pos = np.array(X_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build negative tweet feature set\n",
    "X_neg = []\n",
    "\n",
    "with open('train_neg_processed.txt') as f:\n",
    "    for line in f:\n",
    "        tweet = line.lstrip().split()\n",
    "        X_tweet = []\n",
    "        for word in tweet:\n",
    "            if word in Glove.dictionary and not word in stop_words:\n",
    "                word_vector = Glove.word_vectors[Glove.dictionary.get(word)]\n",
    "                X_tweet.append(word_vector)\n",
    "                \n",
    "        if X_tweet:\n",
    "            X_tweet_processed = process_tweet(X_tweet, method)\n",
    "            X_neg.append(X_tweet_processed)\n",
    "        \n",
    "X_neg = np.array(X_neg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pos = np.ones(X_pos.shape[0])\n",
    "y_neg = -np.ones(X_neg.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_samples_train = -1 \n",
    "\n",
    "# cut samples\n",
    "X_pos_cut = X_pos[:N_samples_train,:]\n",
    "X_neg_cut = X_neg[:N_samples_train,:]\n",
    "\n",
    "# cut targets\n",
    "y_pos_cut = y_pos[:N_samples_train]\n",
    "y_neg_cut = y_neg[:N_samples_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_pos_neg = np.concatenate([X_pos_cut, X_neg_cut])\n",
    "y_pos_neg = np.concatenate([y_pos_cut, y_neg_cut])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_pos_neg)\n",
    "X_pos_neg_scaled = scaler.transform(X_pos_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SVM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C-Support Vector Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC()\n",
    "clf.fit(X_pos_neg_scaled, y_pos_neg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nu-Support Vector Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import NuSVC\n",
    "\n",
    "clf = NuSVC()\n",
    "clf.fit(X_pos_neg_scaled, y_pos_neg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = LinearSVC(penalty='l1', dual = False)\n",
    "clf.fit(X_pos_neg_scaled, y_pos_neg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'D:\\Documents\\etudes\\epfl\\MA1\\cours\\MachineLearning\\Project2\\data\\twitter_datasets_epfl\\short')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = []\n",
    "\n",
    "with open('test_data_no_id_processed.txt') as f:\n",
    "    \n",
    "    for line in f:\n",
    "        \n",
    "        tweet = line.lstrip().split()\n",
    "        X_tweet = []\n",
    "        \n",
    "        for word in tweet:\n",
    "            \n",
    "            if word in Glove.dictionary and not word in stop_words:\n",
    "                \n",
    "                word_vector = Glove.word_vectors[Glove.dictionary.get(word)]\n",
    "                X_tweet.append(word_vector)\n",
    "                \n",
    "        if X_tweet:\n",
    "            \n",
    "            X_tweet_processed = process_tweet(X_tweet, method)\n",
    "            X_test.append(X_tweet_processed)\n",
    "        \n",
    "X_test = np.array(X_test) \n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# standarization\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'D:/Documents/etudes/epfl/MA1/cours/MachineLearning/Project2/data/submissions/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from create_csv_submission import create_csv_submission\n",
    "import time\n",
    "import datetime\n",
    "i = datetime.datetime.now()\n",
    "\n",
    "name = \"sub_\" + time.strftime(\"%d_%m_%Y\") +  \"_%sh_%smin\" % (i.hour, i.minute)\n",
    "#name = \"vrf\"\n",
    "ids_test = range(1, X_test.shape[0]+1)\n",
    "create_csv_submission(ids_test, y_pred, name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
