{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# change path if necessary\n",
    "import sys\n",
    "my_path = r'C:\\Users\\utente\\Documents\\GitHub\\Project2'\n",
    "sys.path.insert(0,my_path + r'/code/Flavio')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "print('TensorFlow imported')\n",
    "\n",
    "import dictionary_helpers as dh\n",
    "\n",
    "# imports\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'C:\\Users\\utente\\Desktop\\data\\twitter_datasets_stanford')\n",
    "\n",
    "glove_dict = dh.build_glove_dict('glove.twitter.27B.25d.txt')\n",
    "\n",
    "print(len(glove_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.84705 , -1.0349  , -0.050419,  0.27164 , -0.58659 ,  0.99514 ,\n",
       "        0.25267 ,  1.6963  ,  0.10313 ,  0.80073 ,  0.74655 , -1.2667  ,\n",
       "       -4.036   , -0.22557 ,  0.16322 , -0.67015 , -0.64812 ,  0.010373,\n",
       "       -0.71889 , -0.74997 ,  0.24862 ,  0.10319 , -1.1732  ,  0.58196 ,\n",
       "        0.33846 ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_dict[',']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191321\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r'C:\\Users\\utente\\Desktop\\data\\twitter_datasets_epfl\\short')\n",
    "#os.chdir(r'C:\\Users\\utente\\Desktop\\data\\twitter_datasets_epfl\\full')\n",
    "\n",
    "tweets = dh.build_tweets_matrix('all_short_processed.txt')\n",
    "        \n",
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'your'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  4.]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([])\n",
    "\n",
    "ciao=np.append(x,[1,4], axis=0)\n",
    "\n",
    "\n",
    "print(ciao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build averages+min+max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191321, 75)\n",
      "[-0.1865898  0.236394   0.330381  -0.508172  -0.6313082 -0.1213108\n",
      "  1.256084  -0.3280116 -0.69894    0.190782  -0.289562   0.6072632 -5.21422\n",
      " -0.1280236 -0.211636   0.387608   0.2012168 -0.0554106 -0.053988\n",
      " -0.2539778  0.139662   0.33784   -0.0941022 -0.0355024  0.0722898\n",
      "  0.088601   0.59108    0.96279    0.19839   -0.092441   0.38567    2.1\n",
      "  0.066642  -0.49794    0.70047    0.43174    1.4784    -3.1591     0.15437\n",
      "  0.52152    0.86374    0.59554    0.88961    1.292      0.33094    0.4516\n",
      "  0.65844    0.54594    0.25164    0.2758    -0.45649   -0.20207   -0.12672\n",
      " -1.1372    -1.247     -0.40767    0.68192   -0.84406   -1.1467    -0.17034\n",
      " -0.87221   -0.013954  -5.995     -0.43106   -0.9154    -0.11314   -0.33704\n",
      " -0.47587   -0.84178   -0.84126   -0.4582    -0.16727   -0.47606   -0.39695\n",
      " -0.16073  ]\n"
     ]
    }
   ],
   "source": [
    "X = np.empty([len(tweets), feat_size*3])\n",
    "\n",
    "for i in range(len(tweets)):\n",
    "    feats = []\n",
    "    for j in range(len(tweets[i])):\n",
    "        if tweets[i][j] in glove_dict:\n",
    "            feats.append(glove_dict[tweets[i][j]])\n",
    "        else:\n",
    "            feats.append([0]*feat_size)\n",
    "\n",
    "    X[i]= np.hstack((np.mean(feats, axis=0), np.amax(feats, axis=0), np.amin(feats, axis=0)))\n",
    "\n",
    "print(X.shape)\n",
    "print(X[9])\n",
    "\n",
    "labels = []\n",
    "\n",
    "for i in range(90233):\n",
    "    labels.append([1,0])\n",
    "for i in range(len(tweets)-90233):\n",
    "    labels.append([0,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "featSet = []\n",
    "\n",
    "for i in range(len(tweets)):\n",
    "    featSet.append([X[i,:].tolist(), labels[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16076666666666664, 0.37080678666666667, -0.15070153333333333, -0.031043000000000022, -0.4298712, -0.30033666666666664, 1.0273436666666667, 1.2202581333333336, -0.8047160000000002, 0.14191302, -0.20511066666666664, 0.07287840000000001, -3.885906666666666, -0.49186399999999997, -0.16666466666666666, -0.22948193333333336, 0.11266059999999999, -0.6132825333333332, -0.6732669333333333, -0.05011833333333333, 0.213364, -0.29275606666666665, -0.08284273333333335, 0.4607126666666667, 0.2680893333333333, 0.62415, 1.1861, 0.61622, 1.0119, 0.084332, 0.12081, 2.2315, 2.6356, 0.0, 0.70047, 0.57517, 1.0839, 0.0, 0.0, 0.31843, 1.3491, 0.57602, 0.0, 0.45495, 0.65635, 0.81819, 0.53068, 1.6392, 1.6294, 1.6189, -0.62645, -0.29194, -1.3632, -1.201, -0.97979, -1.459, -0.025355, -0.73405, -1.7277, -1.1867, -0.8847, -2.0128, -6.0385, -1.2343, -0.94359, -0.94879, -0.71249, -0.9871, -1.3348, -0.76565, -0.55258, -1.1655, -0.62747, -1.6099, -0.88679], [0, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(featSet[190000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(X)\n",
    "del(labels)\n",
    "del(tweets)\n",
    "del(glove_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### crea train_x train_y test_x test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(featSet)\n",
    "\n",
    "featSet = np.array(featSet)\n",
    "\n",
    "testing_size = int(0.1*len(featSet))\n",
    "\n",
    "train_x = list(featSet[:,0][:-testing_size])\n",
    "train_y = list(featSet[:,1][:-testing_size])\n",
    "\n",
    "test_x = list(featSet[:,0][-testing_size:-1])\n",
    "test_y = list(featSet[:,1][-testing_size:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed out of 50 loss: 3957.93492892\n",
      "Epoch 2 completed out of 50 loss: 344.349525378\n",
      "Epoch 3 completed out of 50 loss: 190.23245853\n",
      "Epoch 4 completed out of 50 loss: 130.749658352\n",
      "Epoch 5 completed out of 50 loss: 91.6765143982\n",
      "Epoch 6 completed out of 50 loss: 67.9285935868\n",
      "Epoch 7 completed out of 50 loss: 50.563290829\n",
      "Epoch 8 completed out of 50 loss: 40.0175684578\n",
      "Epoch 9 completed out of 50 loss: 32.2218884737\n",
      "Epoch 10 completed out of 50 loss: 25.9500266801\n",
      "Epoch 11 completed out of 50 loss: 22.5057114224\n",
      "Epoch 12 completed out of 50 loss: 18.3631578137\n",
      "Epoch 13 completed out of 50 loss: 16.8957597028\n",
      "Epoch 14 completed out of 50 loss: 14.3127698676\n",
      "Epoch 15 completed out of 50 loss: 11.9556188734\n",
      "Epoch 16 completed out of 50 loss: 11.4944361826\n",
      "Epoch 17 completed out of 50 loss: 9.82760720857\n",
      "Epoch 18 completed out of 50 loss: 9.68349231366\n",
      "Epoch 19 completed out of 50 loss: 8.71518126732\n",
      "Epoch 20 completed out of 50 loss: 7.45344034489\n",
      "Epoch 21 completed out of 50 loss: 6.72414327174\n",
      "Epoch 22 completed out of 50 loss: 7.48527103529\n",
      "Epoch 23 completed out of 50 loss: 5.39403551823\n",
      "Epoch 24 completed out of 50 loss: 5.64335986028\n",
      "Epoch 25 completed out of 50 loss: 5.39035202495\n",
      "Epoch 26 completed out of 50 loss: 4.59196625091\n",
      "Epoch 27 completed out of 50 loss: 3.79922325567\n",
      "Epoch 28 completed out of 50 loss: 5.18730900633\n",
      "Epoch 29 completed out of 50 loss: 4.32587157071\n",
      "Epoch 30 completed out of 50 loss: 4.53258825436\n",
      "Epoch 31 completed out of 50 loss: 3.08814042707\n",
      "Epoch 32 completed out of 50 loss: 2.67113550295\n",
      "Epoch 33 completed out of 50 loss: 3.91282468474\n",
      "Epoch 34 completed out of 50 loss: 3.39596245509\n",
      "Epoch 35 completed out of 50 loss: 2.0944242579\n",
      "Epoch 36 completed out of 50 loss: 2.76808671115\n",
      "Epoch 37 completed out of 50 loss: 3.63448993429\n",
      "Epoch 38 completed out of 50 loss: 2.78618820281\n",
      "Epoch 39 completed out of 50 loss: 2.42375729827\n",
      "Epoch 40 completed out of 50 loss: 3.01172462827\n",
      "Epoch 41 completed out of 50 loss: 1.68362371502\n",
      "Epoch 42 completed out of 50 loss: 1.962150423\n",
      "Epoch 43 completed out of 50 loss: 2.89639476655\n",
      "Epoch 44 completed out of 50 loss: 2.31278959417\n",
      "Epoch 45 completed out of 50 loss: 1.71984260495\n",
      "Epoch 46 completed out of 50 loss: 1.59712831569\n",
      "Epoch 47 completed out of 50 loss: 2.49975359478\n",
      "Epoch 48 completed out of 50 loss: 1.02040985571\n",
      "Epoch 49 completed out of 50 loss: 2.00571088157\n",
      "Epoch 50 completed out of 50 loss: 2.28795776623\n",
      "Accuracy: 0.997177\n"
     ]
    }
   ],
   "source": [
    "n_inputs = 75\n",
    "n_nodes_hl1 = 75\n",
    "n_nodes_hl2 = 30\n",
    "n_nodes_hl3 = 15\n",
    "\n",
    "n_classes = 2\n",
    "batch_size = 250\n",
    "\n",
    "\n",
    "# input feature size = 50feats x 20words = 2000 \n",
    "x = tf.placeholder('float', [None, n_inputs])\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "#DEFINITION OF THE NETWORK:\n",
    "#1) random initialization of the various layer with respectives dimensions, the layer is a DICTIONARY of weigths and biases\n",
    "#2) application of weights and biases and activation function to each layer\n",
    "\n",
    "\n",
    "def neural_network_model(data):\n",
    "    # input_data * weights + biases\n",
    "\n",
    "    #tf.random_normal returns an array of the specified dimension filled with randomly distributed numbers\n",
    "    #tf.Variable = object to be created that works in the tensorflow session, just a wrapper.\n",
    "    hidden_l1 = {'weights': tf.Variable(tf.random_normal([n_inputs, n_nodes_hl1])),\n",
    "    'biases': tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "    hidden_l2 = {'weights': tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "    'biases': tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "    hidden_l3 = {'weights': tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "    'biases': tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "    output_l  = {'weights': tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "    'biases': tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "    l1 = tf.add(tf.matmul(data, hidden_l1['weights']), hidden_l1['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1, hidden_l2['weights']), hidden_l2['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2, hidden_l3['weights']), hidden_l3['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "\n",
    "    output = tf.add(tf.matmul(l3, output_l['weights']), output_l['biases'])\n",
    "    \n",
    "    return output\n",
    "\n",
    "def train_neural_network(x):\n",
    "    prediction = neural_network_model(x)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))   # v1.0 changes\n",
    "    # optimizer value = 0.001, Adam similar to SGD\n",
    "    # everything is updating through this optimizer, i don't know how.... problem!? maybe better not to know\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    epochs_no = 50\n",
    " \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())   # v1.0 changes\n",
    "  \n",
    "        # training\n",
    "        for epoch in range(epochs_no):\n",
    "            epoch_loss = 0\n",
    "\n",
    "            #batching\n",
    "            i = 0\n",
    "            while i < len(train_x):\n",
    "                start = i\n",
    "                end = i+batch_size\n",
    "                \n",
    "                batch_x = np.array(train_x[start:end])\n",
    "                batch_y = np.array(train_y[start:end])\n",
    "                \n",
    "                _, c = sess.run([optimizer, cost], feed_dict = {x: batch_x, y: batch_y})\n",
    "                # code that optimizes the weights & biases\n",
    "                epoch_loss += c\n",
    "                i+=batch_size\n",
    "                \n",
    "            print('Epoch', epoch+1, 'completed out of', epochs_no, 'loss:', epoch_loss)\n",
    "\n",
    "        # testing\n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print('Accuracy:', accuracy.eval({x: test_x, y: test_y}))\n",
    "\n",
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTTTTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
